{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROB311 - TP2\n",
    "***Brice Tayart***\n",
    "\n",
    "2nd Lab assignment for the *Machine learning for Robotics* course at ENSTA by Prof. Adriana Tapus.\n",
    "\n",
    "The subject is optimization methods to seek the optimal policy in a Markov Decision Process, namely the Value iteration and Policy iteration methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Screenshot_ROB311_TP2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In the figure above, the states are depicted by circles (S0, S1, S2, and S3) and the associated actions are indicated on the arrows: a0, a1, and a2. The transition functions for all the actions are shown\n",
    "below.*\n",
    "\n",
    "$$T(S,a_0,S') = \\left(\n",
    "\\begin{array}{cccc}\n",
    "  0 &   0 &   0 &   0 \\\\\n",
    "  0 & 1-x &   0 &   x \\\\\n",
    "1-y &   0 &   0 &   y \\\\\n",
    "  1 &   0 &   0 &   0\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$T(S,a_1,S') = \\left(\n",
    "\\begin{array}{cccc}\n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "$$T(S,a_1,S') = \\left(\n",
    "\\begin{array}{cccc}\n",
    "0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{array}\n",
    "\\right)$$\n",
    "\n",
    "*Each of the parameters $x$ and $y$ are in the interval $[0, 1]$, and the discounted factor $\\gamma \\in [0,1[$*\n",
    "\n",
    "*The reward is:*\n",
    "$$R(S) = \\left\\{\n",
    "\\begin{array}{cl}\n",
    "10 & \\text{for state } S_3 \\\\\n",
    "1 & \\text{for state } S_2 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "*Enumerate all the possible policies*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For state $S_0$, there are two possible actions $a_1$ or $a_2$. For all other states $S_1$, $S_2$ and $S_3$, only a default action $a_0$ is available.\n",
    "\n",
    "Therefore, a policy is entierly defined by the action taken for state $S_0$. There are two policies available:\n",
    "\n",
    "$$ \\pi_1(s) =\n",
    "\\begin{cases}\n",
    "a_1 & \\mbox{for } s = S_0 \\\\\n",
    "a_0 & \\mbox{for } s \\in \\{S_i| i\\in\\{1,2,3\\}\\}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$ \\pi_2(s) =\n",
    "\\begin{cases}\n",
    "a_2 & \\mbox{for } s = S_0 \\\\\n",
    "a_0 & \\mbox{for } s \\in \\{S_i| i\\in\\{1,2,3\\}\\}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "*Write the equation for each optimal value function for each state*\n",
    "$(V^*(S_0),V^*(S_1),V^*(S_2),V^*(S_3))$\n",
    "\n",
    "*Reminder:*\n",
    "$$V^*(S) = R(S) + \\max_a \\gamma \\sum_{S'}T(S,a,S')V^*(S')$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equations are as follows:\n",
    "\n",
    "$V^{*}(S_0) = 0 + \\gamma \\min \\left(V^{*}(S_1), V^{*}(S_2)\\right)$\n",
    "\n",
    "$V^{*}(S_1) = 0 + \\gamma \\left( x\\cdot V^{*}(S_3) + (1-x)\\cdot V^{*}(S_1)\\right)$\n",
    "\n",
    "$V^{*}(S_2) = 1 + \\gamma \\left( y\\cdot V^{*}(S_3) + (1-y)\\cdot V^{*}(S_0)\\right)$\n",
    "\n",
    "$V^{*}(S_3) = 10 + \\gamma V^{*}(S_0)$\n",
    "\n",
    "Since the only choice is in $S_0$, we see that only the value $V^{*}(S_0)$ depends on a *max* between two outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "*Is there a value for x, that $\\forall y \\in [0,1], \\forall \\gamma \\in [0,1[, \\pi^*(s_0) = a_2$.*\n",
    "\n",
    "*Justify your answer.*\n",
    "*Reminder:*\n",
    "$$\\pi^*(S) = \\arg \\max_a \\sum_{S'}T(S,a,S')V^*(S')$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with policy $\\pi_1$, is that the agent:\n",
    "- transitions from state $S_0$ to state $S_1$\n",
    "- remains \"trapped\" in state $S_1$ for some time until transitioning to $S_3$ with a probability $x$. The average number of turns spent in state $S_1$ is $1/x$. During that time, no reward is won and any prior utility is discounted at a rate $\\gamma$\n",
    "- gets a reward of **10** and return to state $S_0$, completing the cycle\n",
    "\n",
    "Intuitively, the utility value $V^{\\pi_1}(S_0)$ decreases when $x$ decreases - i.e. the time spent in state $S_1$ increases - and reaches $0$ if $x=0$, as the agent will be trapped forever in state $S_1$.\n",
    "\n",
    "Indeed, equation for $V(S_1)$ car be reformulated as:\n",
    "$$V(S_1) = \\frac{\\gamma x}{1-\\gamma+\\gamma x} \\cdot V(S_3)$$\n",
    "\n",
    "which shows that:\n",
    "- $V(S_1) = 0$, regardless of $\\gamma$, if $x = 0$\n",
    "- $V(S_1) = 0$, regardless of $x$, if $\\gamma = 0$\n",
    "- $V(S_1) = V(S_3)$, regardless of $x$, if $\\gamma = 1$\n",
    "- $V(S_1) = \\gamma V(S_3)$ if $x = 1$\n",
    "- for a given $\\gamma$ and $V(S_3)$, $V(S_1)$ is greatest for $x=1$ since\n",
    "$\\frac{\\partial}{\\partial x} \\left(\\frac{\\gamma x}{1-\\gamma+\\gamma x}\\right) = \\frac{\\gamma(1-\\gamma)}{(1-\\gamma+\\gamma x)^2} \\ge 0$\n",
    "\n",
    "On the other hand, all rewards are positive so that the utility value of any state is greater than its reward: $V^{*}(S_i)\\ge R(S_i)$.\n",
    "For all policies, $V(S_2)\\ge 1$, so $V^{\\pi_2}(S_0) \\ge \\gamma$.\n",
    "\n",
    "In the case where $x=0$, policy $\\pi_2$ will always prevail over policy $\\pi_1$, regardless of *y* and $\\gamma$, i.e. $\\pi^*(S_0)=a_2$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Further substitutions using:\n",
    "$$V^{\\pi_1}(S_0) = \\gamma V^{\\pi_1}(S_1)$$\n",
    "$$V^{\\pi_1}(S_3) = 10 + \\gamma V^{\\pi_1}(S_0)$$\n",
    "\n",
    "lead to the equation for $V^{\\pi_1}(S_0)$:\n",
    "$$V^{\\pi_1}(S_0) = \\frac{10 \\gamma ^2 x}{1-\\gamma(1-x)-\\gamma^3x}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "*Is there a value for *y*, such that $\\forall x > 0, \\forall \\gamma \\in [0,1] , \\pi^âˆ—(s_0) = a_1$.*\n",
    "\n",
    "*Justify your answer.*\n",
    "*Reminder:*\n",
    "$$\\pi^*(s) = \\arg \\max_a \\sum_{S'}T(S,a,S')V^*(S')$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with policy $\\pi_2$, is that the agent:\n",
    "- transitions from state $S_0$ to state $S_2$ and gets an immediate reward of 1\n",
    "- with a probability *y*, transitions to state $S_3$, gets a reward of **10** and return to state $S_0$, completing the cycle\n",
    "- with a probability $1-y$, returns to state $S_0$ without any further reward\n",
    "\n",
    "Intuitively, we see that there is minimum discount rate $\\gamma$ below which policy $\\pi_2$ has to be optimal.\n",
    "\n",
    "From question 3, the best possible result of action $a_1$ is the sequence $S_0 (+0) \\rightarrow S_1(+0) \\rightarrow S_3(+10) \\rightarrow S_0(+0)$ (which happens with probability 1 if $x=1$).\n",
    "\n",
    "On the other hand, action $a_2$ produces the sequence $S_0(+0) \\rightarrow S_2(+1) \\rightarrow S_3(+10) \\rightarrow S_0(+0)$ with probability *y*. This gives a higher reward than achievable with action $a_1$.\n",
    "\n",
    "It can also produce the sequence $S_0(+0) \\rightarrow S_2(+1) \\rightarrow S_0$, which has a higher reward than some hypothetical sequence $S_0(+0) \\rightarrow S_2(+1) \\rightarrow (wait) \\rightarrow S_0$. If $\\gamma < 0.1$, a reward of 1 at the second step of the sequence will have a higher value than a rewards of 10 obtained at the third step, and that sequence will also beat the best result achievable with $a_1$\n",
    "\n",
    "Formally:\n",
    "\n",
    "$$\\begin{array}{rcl} \n",
    "V^{\\pi_1}(S_0)  & = & \\frac{10 \\gamma ^2 x}{1-\\gamma(1-x)-\\gamma^3x} \\le \\frac{10 \\gamma ^2}{1-\\gamma^3}\\\\\n",
    " & \\mbox{and} & \\\\\n",
    "V^{\\pi_2}(S_0)  & = & \\gamma V^{\\pi_2}(S_2) \\\\\n",
    "                & = & \\gamma(1 + y V^{\\pi_2}(S_3) + (1-y) V^{\\pi_2}(S_0)) \\\\\n",
    "V^{\\pi_2}(S_0)  & = & y(10\\gamma^2+\\gamma^3V^{\\pi_2}(S_0)) + (1-y)(\\gamma^2V^{\\pi_2}(S_0))+\\gamma\n",
    "\\end{array}$$\n",
    "\n",
    "Under the condition that $10 \\gamma (1-y) \\ge 1$, and since $\\gamma \\in [0,1]$ we also have:\n",
    "$$\\begin{array}{rcl} \n",
    "(1-y)\\gamma^2V^{\\pi_2}(S_0) + \\gamma & \\ge & (1-y)\\gamma^3V^{\\pi_2}(S_0)+\\gamma \\\\\n",
    "                                     & \\ge & (1-y)\\gamma^3V^{\\pi_2}(S_0)+10 \\gamma^2 (1-y) \\\\\n",
    "(1-y)\\gamma^2V^{\\pi_2}(S_0) + \\gamma & \\ge & (1-y)(10 \\gamma^2 +\\gamma^3V^{\\pi_2}(S_0)) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "This gives a common lower bound to both terms in the expression of $V^{\\pi_2}(S_0)$:\n",
    "$$\\begin{array}{rcl} \n",
    "V^{\\pi_2}(S_0)  & = & y(10\\gamma^2+\\gamma^3V^{\\pi_2}(S_0)) + (1-y)(\\gamma+\\gamma^2V^{\\pi_2}(S_0)) + \\gamma\\\\\n",
    "                & \\ge & y(10\\gamma^2+\\gamma^3V^{\\pi_2}(S_0)) + (1-y)(10\\gamma^2+\\gamma^3V^{\\pi_2}(S_0))\\\\\n",
    "V^{\\pi_2}(S_0)  & \\ge & 10\\gamma^2+\\gamma^3V^{\\pi_2}(S_0) \\\\\n",
    "(1-\\gamma^3)V^{\\pi_2}(S_0)  & \\ge & 10\\gamma^2 \\\\\n",
    "V^{\\pi_2}(S_0)  & \\ge & \\frac{10\\gamma^2}{1-\\gamma^3} \\\\\n",
    "V^{\\pi_2}(S_0)  & \\ge & V^{\\pi_1}(S_0) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "$\\pi_2$ is thus an optimal policy if sufficient condition $10 \\gamma (1-y) \\ge 1$ is met, regardless of *x*. In particular, it is sufficient to have $y \\ge 9/10$ or $\\gamma \\le 1/10$. \n",
    "\n",
    "Thus, regardless of the value of y and x, $\\pi_2$ is an optimal policy if $\\gamma \\le 0.1$. In general, for a given value of *y*, $\\pi_2$ is an optimal policy if $\\gamma \\le 0.1/(1-y)$\n",
    "\n",
    "Qualitatively, policy $\\pi_2$ will be favored over $\\pi_1$ when:\n",
    "- *x* is small (i.e. the agent is expected to remain trapped in state $S_1$ for a long time)\n",
    "- $\\gamma$ is small, because potential large long-term rewards through action $a_1$ lose value compared to the small immediate reward gained through action $a_2$\n",
    "- *y* is large\n",
    "\n",
    "As a conclusion, there is no pair of values $(x,y) \\in [0,1]^2$ such that $\\pi^*(S_0)=a_1$ for all $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Value iteration\n",
    "*Using $x=y=0.25$ and $\\gamma = 0.9$ , calculate the $\\pi^*$ and $V^*$ for all states.*\n",
    "\n",
    "*Implement value iteration.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking utility values and best policy for parameters :\n",
      "    x = 0.25\n",
      "    y = 0.25\n",
      "gamma = 0.9\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "x = .25\n",
    "y = .25\n",
    "gamma = .9\n",
    "\n",
    "# Define nodes\n",
    "rewards = np.array([0., 0., 1., 10.])\n",
    "\n",
    "# Define actions and their transition matrices\n",
    "# state -> action -> transition matrix to state\n",
    "actions = {\n",
    "    0: {1: np.array([0,1,0,0]), \n",
    "        2: np.array([0,0,1,0])},\n",
    "    1: {0: np.array([0,1-x,0,x])},\n",
    "    2: {0: np.array([1-y,0,0,y])},\n",
    "    3: {0: np.array([1,0,0,0])},\n",
    "    }\n",
    "\n",
    "# Print values:\n",
    "print(\"Seeking utility values and best policy for parameters :\")\n",
    "print(f\"    x = {x}\")\n",
    "print(f\"    y = {y}\")\n",
    "print(f\"gamma = {gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration\n",
    "The goal is to iteratively seek the values of each node. Once this has been done, the choice of an optimal policy at each node is trivial.\n",
    "\n",
    "For each node, the expected utility of all possible actions is computed according to the other nodes' current value. The utility of the best action (i.e. the expected discounted value for the next node) is used in order to update the node value. This is repeated until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Functions used for value iteration:\n",
    "# update_policy : seeks the best policy and the expected outcome\n",
    "#     of the best policy for each node (used for value and policy iteration)\n",
    "# update_utility : updates the utility values according to the best policy\n",
    "# value_iteration : repeats update_utility until convergence\n",
    "\n",
    "def update_utility(utility, rewards, actions, discount):\n",
    "    '''\n",
    "    Function that performs value iteration by updating utility\n",
    "    with rewards + discounted expected outcome of best policy\n",
    "    '''\n",
    "    _, optimal_choice = update_policy(utility, actions)\n",
    "    return rewards + discount*optimal_choice\n",
    "\n",
    "def update_policy(utility, actions):\n",
    "    '''\n",
    "    Function that seeks the best policy based on a given utility\n",
    "    It also returns the expected outcome of the actions taken\n",
    "    when following the best policy\n",
    "    '''\n",
    "    \n",
    "    #init (with zeros (because )we have only positive utilities)\n",
    "    optimal_choice = np.zeros(rewards.shape, dtype='float')-np.inf\n",
    "    best_policy = np.zeros(rewards.shape, dtype='int')\n",
    "\n",
    "    #Loop over states and available actions\n",
    "    for st,act in actions.items():\n",
    "        for a,transition in act.items():\n",
    "            \n",
    "            # Compute expected utility based on transition probabilities\n",
    "            u = np.sum(transition * utility)\n",
    "            \n",
    "            # Update if new action is optimal (seek the max and save arg max)\n",
    "            if u > optimal_choice[st]:\n",
    "                optimal_choice[st] = u\n",
    "                best_policy[st] = a    \n",
    "    return (best_policy, optimal_choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(rewards, actions, discount, max_iter = 100, thres = 1e-3):\n",
    "    '''\n",
    "    Value iteratoin algorithm:\n",
    "    repeatedely update utility until convergence\n",
    "    '''\n",
    "    # initialize utility with rewards\n",
    "    utility = rewards.copy()\n",
    "    old_rms = -1.0\n",
    "    new_rms = np.sum(utility**2)\n",
    "    ii = 0\n",
    "\n",
    "    # loop until convergence\n",
    "    while ii<max_iter and np.abs(1 - new_rms/old_rms)>=thres :\n",
    "        old_rms = new_rms\n",
    "        utility = update_utility(utility, rewards, actions, discount)\n",
    "        new_rms = np.sum(utility**2)\n",
    "        ii += 1\n",
    "\n",
    "    return (utility, ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Result after 50 iterations for value iteration:\n",
      "\n",
      "The utility values for the states are:\n",
      "14.111, 15.687, 15.623, 22.692\n",
      "\n",
      "The optimal policy is\n",
      "State 0 --> action 1\n",
      "State 1 --> action 0\n",
      "State 2 --> action 0\n",
      "State 3 --> action 0\n"
     ]
    }
   ],
   "source": [
    "#%% Run the value iteration algorithm and print results\n",
    "\n",
    "utility,niter = value_iteration(rewards, actions, gamma)\n",
    "optimal_policy, dummy = update_policy(utility, actions)\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Result after %d iterations for value iteration:\\n\"%niter)\n",
    "\n",
    "print(\"The utility values for the states are:\")\n",
    "print(\", \".join([\"%.3f\"%v for v in utility]))\n",
    "\n",
    "print(\"\\nThe optimal policy is\")\n",
    "print(\"\\n\".join(\"State %d --> action %d\"%(st,act) for st,act in enumerate(optimal_policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration\n",
    "For policy iteration, a policy is picked and used to evaluate the nodes' value. Then, the policy is revised and the process repeated until convergence has been reached.\n",
    "\n",
    "Here, the value of the nodes is evaluated by iterating. What is done is more or less the same as value iteration, only we do not attempt to find the optimal policy at each iteration and just loop to get values. There are however analytical methods to compute the values of the nodes that could also be used (for a fixed policy, this is just a system of equations to solve).\n",
    "\n",
    "Once the values are known, the optimal policy is chosen as a new policy and the values re-computed, and so on until a stabe policy choice has been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Policy iteration\n",
    "# evaluate_policy : repeatedely updates the utility values based on a given policy,\n",
    "#     until convergence has been obtained\n",
    "# update_policy : seeks the best policy given utility values\n",
    "# policy_iteration : alternates between evaluating and updating policy until a stable policy has been found\n",
    "\n",
    "def evaluate_policy(policy, rewards, discount, utility_init=None, max_iter = 100, thres = 1e-3):\n",
    "    '''\n",
    "    Evaluate policy\n",
    "    This loops to evaluate the utility values linked to a policy\n",
    "    The policy is fixed\n",
    "    '''\n",
    "    if utility_init is None:\n",
    "        utility = rewards.copy()\n",
    "    else:\n",
    "        utility = utility_init.copy()\n",
    "    \n",
    "    #initialize variables\n",
    "    ii = 0\n",
    "    new_rms = np.sum(utility ** 2)\n",
    "    old_rms = -1\n",
    "    new_utility = np.zeros(utility.shape, dtype='float')\n",
    "    \n",
    "    #Loop until convergence\n",
    "    while ii<max_iter and np.abs(1-new_rms/old_rms) > thres:\n",
    "        \n",
    "        #Loop over states and update utility based on policy\n",
    "        for st,act in actions.items():\n",
    "            transition = act[policy[st]]\n",
    "            new_utility[st] = rewards[st] + \\\n",
    "                              discount*np.sum(transition*utility)\n",
    "        \n",
    "        # swap (new utility is not re-used, we just keep an initialized array)\n",
    "        tmp = utility\n",
    "        utility = new_utility\n",
    "        new_utility = tmp\n",
    "        \n",
    "        # update rms\n",
    "        old_rms = new_rms\n",
    "        new_rms = np.sum(utility ** 2)\n",
    "    \n",
    "    return utility\n",
    "\n",
    "def policy_iteration(rewards, actions, discount, init_policy = None,\n",
    "                     max_iter_policy = 10, max_iter_evaluate = 100):\n",
    "    #initialize policy (if not given)\n",
    "    if init_policy is None:\n",
    "        init_policy = np.zeros(rewards.shape, dtype='int')\n",
    "        #for each state, pick one action\n",
    "        for st,act in actions.items():\n",
    "            init_policy[st]=list(act)[0]\n",
    "    \n",
    "    #initialize some variables\n",
    "    old_policy = tuple()\n",
    "    policy = init_policy.copy()\n",
    "    utility = None\n",
    "    ii=0\n",
    "    \n",
    "    #alternate policy evaluation and policy selection until stable\n",
    "    while ii<max_iter_policy and old_policy != tuple(policy):\n",
    "        old_policy = tuple(policy)\n",
    "        utility = evaluate_policy(policy, rewards, discount, utility, max_iter = max_iter_evaluate)\n",
    "        policy, dummy = update_policy(utility, actions)\n",
    "        ii += 1\n",
    "        \n",
    "    return utility, policy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Result for policy iteration :\n",
      "\n",
      "The utility values for the states are:\n",
      "14.109, 15.685, 15.621, 22.690\n",
      "\n",
      "The optimal policy is\n",
      "State 0 --> action 1\n",
      "State 1 --> action 0\n",
      "State 2 --> action 0\n",
      "State 3 --> action 0\n"
     ]
    }
   ],
   "source": [
    "utility,optimal_policy = policy_iteration(rewards, actions, gamma,\n",
    "                                          init_policy=np.array([2,0,0,0],dtype='int'))\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Result for policy iteration :\\n\")\n",
    "\n",
    "print(\"The utility values for the states are:\")\n",
    "print(\", \".join([\"%.3f\"%v for v in utility]))\n",
    "\n",
    "print(\"\\nThe optimal policy is\")\n",
    "print(\"\\n\".join(\"State %d --> action %d\"%(st,act) for st,act in enumerate(optimal_policy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical values\n",
    "The theoretical value for $V^{\\pi_1}(S_0)$ is: $$V^{\\pi_1}(S_0) = \\frac{10\\gamma^2x}{1-\\gamma (1-x)-\\gamma^3 x}$$\n",
    "\n",
    "The theoretical value for $V^{\\pi_2}(S_0)$ is: $$V^{\\pi_2}(S_0) = \\frac{10\\gamma^2y+\\gamma}{1-\\gamma^2 (1-y)-\\gamma^3 y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Theoretical results:\n",
      "\n",
      "The theoretical utility values for the states are:\n",
      "14.186, 15.762, 15.698, 22.767\n",
      "\n",
      "The optimal policy is\n",
      "State 0 --> action 1\n",
      "State 1 --> action 0\n",
      "State 2 --> action 0\n",
      "State 3 --> action 0\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------\")\n",
    "print(\"Theoretical results:\\n\")\n",
    "\n",
    "# Theoretical values when using stategy pi_1 (i.e. use action a_1 in state S0)\n",
    "V0 = (10*(gamma**2)*x) / (1 - gamma + gamma*x - (gamma**3)*x)\n",
    "V3 = 10 + gamma*V0\n",
    "V1 = gamma*x/(1-gamma+gamma*x) * V3\n",
    "V2 = 1 + gamma*(y*V3 + (1-y)*V0)\n",
    "\n",
    "# Theoretical values when using stategy pi_2 (i.e. use action a_2 in state S0)\n",
    "U0 = (gamma + 10*(gamma**2) * y) / (1 - (gamma**2)*(1-y) - (gamma**3)*y)\n",
    "U3 = 10 + gamma*U0\n",
    "U1 = gamma*x/(1-gamma+gamma*x) * U3\n",
    "U2 = 1 + gamma*(y*U3 + (1-y)*U0)\n",
    "\n",
    "if V0>U0:\n",
    "    print(\"The theoretical utility values for the states are:\")\n",
    "    print(\", \".join([\"%.3f\"%v for v in [V0, V1, V2, V3]]))\n",
    "    print(\"\\nThe optimal policy is\")\n",
    "    print(\"State 0 --> action 1\")\n",
    "else:\n",
    "    print(\"The theoretical utility values for the states are:\")\n",
    "    print(\", \".join([\"%.3f\"%v for v in [U0, U1, U2, U3]]))\n",
    "    print(\"\\nThe optimal policy is\")\n",
    "    print(\"State 0 --> action 2\")\n",
    "print('\\n'.join('State %d --> action 0'%(st) for st in range(1,4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
